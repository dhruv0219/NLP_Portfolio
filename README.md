# NLP_Portfolio
Portfolio for NLP projects

## Portfolio 0

This is an introduction into NLP and how my interests align with the field. You can see the [Overview of NLP here](Overview_of_NLP.pdf).

## Homework 1

This program takes in data and processes the text to make it readable and presentable using regex. After the data is processed it is then presented to the user. You can see the [file here](Homework1).

## Homework 2

This program takes in a file and collects the 50 most common words out of the text and then allows the user to play a guess game where they try to guess the word. The program utilizes nltk to lemmatize each word and allow for each word to be unique. It also allows us to calculate what words are nouns and use those words as the main portion of the guessing game. You can see the [file here](Homework2).

## Homework 3

This file looks into the use of WordNet and some of the other tools associated with it such as Lesk algorithm, Wu Palmer, SentiWordNet, Collocations and many other tools. This file was originally a google colab notebook, but was converted into a pdf and uploaded here. You can see the [file here](Homework3_Dxt180017ipynb.pdf).

## Homework 4

This program looks into the use of N-Grams as well as using other tools and metrics to calculate accuracies. The program takes in different languages and 
runs them as different N-Gram models as both bigrams and unigrams. From there the program runs a laplace formula to figure out what language the test sentence is going to be based off of the bank of bigrams and unigrams. You can see the [file here](Homework4).

## Parsing Sentences

This paper takes a look at different parsing techniques on a sentence created by me. The different parsing techniques that were used were PSG parsing tree, Dependency Parsing and Semantic Role Labeling. Along with this I give my thoughts on pros and cons of the different techniques. You can see the [file here](Parsing-Sentences.pdf)

## Homework 5

This program uses a web crawler to scrape different websites information on Lionel Messi. The web crawler looks through a starter site then looks at different links and routing pages from there until it comes up with 15 relevant URL sites. Once the websites have been found, the information on the website is scraped and stored in different text files. From there, the websites sentences are tokenized and later the top 25 terms are found. From there we manually selected the top values and created a knowledge base using this data. The knowledge base is later used for a chatbot. You can see the [file here](Homework5)
